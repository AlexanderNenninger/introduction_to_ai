{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need these packages later\n",
    "\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to get help:\n",
    "\n",
    "# help(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Hands-On\n",
    "\n",
    "<img src=\"assets/setosa.bmp\" alt=\"Iris Setosa\" style=\"width: 200px;\"/>\n",
    "<img src=\"assets/versicolor.bmp\" alt=\"Iris Versicolor\" style=\"width: 200px;\"/>\n",
    "<img src=\"assets/virginica.bmp\" alt=\"Iris Virginica\" style=\"width: 200px;\"/>\n",
    "<img src=\"assets/petal_sepal.bmp\" alt=\"Petal and Sepal\" style=\"width: 200px;\"/>\n",
    "\n",
    "## Steps\n",
    "\n",
    "### Import\n",
    "First, we need to load our data from whereever it is stored. Typically these are CSV files, relational databases, or folder structures. In this step, we need to decide, which data we want to use, how to map this data into a Python data structure. We also verify completeness, check for logical errors, etc. If necessary, we request additional metadata, like column descriptions, measurement units, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO\n",
    "\n",
    "+ `read` the dataset form `assets/Iris.csv` using pandas (previously imported as `pd`)\n",
    "+ Check that there are no invalid colums/rows (youn want `info` about your dataset...)\n",
    "+ We want to create a model, which can predict the *species* of each plant. Describe the meaning of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: iris = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of each species are in the dataset? (Hint: `count` `values`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris[\"Species\"] ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the first 5 entries of iris\n",
    "# iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "Make a scatterplot of *petal width vs petal length* and a scatterplot of *sepal width vs sepal length* using `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seaborn. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a violin plot for each of *sepal length*, *sepal width*, *petal length*, *petal width*, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pairplot is a really useful tool to get a complete overview of a dataset\n",
    "# seaborn.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3, diag_kind=\"kde\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "Before we do *anything* with our data, we define our target column, \"Species\", and separate out a *test sample*. We have 150 samples in total. Around 50 samples should suffice for checking our prediction quality. We will not touch our test sample until the very end of our analysis.\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "+ Make a dataframe with all of `['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']` and name it `x`.\n",
    "+ Make a dataframe wit the target column `['Species']` and name it `y`.\n",
    "+ Use `train_test_split` to generate a train dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = ...\n",
    "# y = ...\n",
    "\n",
    "# x_train, x_test, y_train, y_test = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Finally, we can apply machine learning to our dataseet.\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "+ Fit the `DecisionTreeClassifier` to `x_train` and `y_train`\n",
    "+ Predict the species of each sample in `x_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DesiDecisionTreeClassifier()\n",
    "\n",
    "# Train classifier\n",
    "# clf.fit( ... )\n",
    "\n",
    "# Test Classifier\n",
    "# y_pred = clf.predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "+ Generate a confusion matrix\n",
    "+ calculate accuracy\n",
    "+ calculate recall\n",
    "+ calculate presicion\n",
    "+ Are the results satisfying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate Pipeline\n",
    "# species = iris[\"Species\"].unique()\n",
    "# cf_matrix = confusion_matrix(y_test, y_pred, labels=species)\n",
    "\n",
    "# # accuracy = (true positive + true negative) / (true positive + true negative + false psoitive + false negative)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# # recall = true positive / (true positive + false negative)\n",
    "# recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# # precision = true positive / (true_positive + false positive)\n",
    "# precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.1%}\")\n",
    "# print(f\"Recall: {recall:.1%}\")\n",
    "# print(f\"Precision: {precision:.1%}\")\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize=(12,10))\n",
    "# seaborn.heatmap(cf_matrix/cf_matrix.sum(), annot=True, fmt='.1%', xticklabels=species, yticklabels=species, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already Done? \n",
    "\n",
    "Improve prediction accuracy. Can you reach 97%?\n",
    "\n",
    "Already done with that, too?\n",
    "\n",
    "In the folder `./titanic`, there are two files:\n",
    " + `train_data.csv`\n",
    " + `test_data.csv`\n",
    "\n",
    "Predict for each passenger, whether they survived. What biases can you spot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb3fd4630ad1fcd40bc3bf8c100672ac8a13ae39fb9c88ccbc510518b4de0f1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
